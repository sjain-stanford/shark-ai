// Copyright 2025 Advanced Micro Devices, Inc.
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

//===----------------------------------------------------------------------===//
//
// This file contains the `TensorAttr` class definition for all compile-time
// constant metadata pertaining to tensors.
//
// A note on memory formats, strides and contiguity of tensors:
//
// A contiguous tensor has the same logical and physical (in-memory) layout and
// doesn't require strides to define memory access patterns. But if we were to
// assign strides, a contiguous tensor of shape (n, c, h, w) may be accessed in
// row-major order using strides (c*h*w, h*w, w, 1) - reversed cumulative
// product of its dimensions. Likewise a contiguous tensor of shape (n, h, w, c)
// may be accessed in row-major order using strides (h*w*c , w*c, c, 1).
// Essentially, strides of contiguous row-major tensors are monotonically
// decreasing (increasing for column major) and the last dimension (first for
// column major) always has a stride of 1.
//
// A non-contiguous tensor has a physical (in-memory) layout that is different
// from its logical layout. Strides can be used to represent any valid
// permutation of the tensor's dimensions. For example, a tensor of logical
// shape (n, c, h, w) may be stored with various physical layouts:
//   - Channels-last (NHWC): strides (h*w*c, 1, w*c, c)
//   - Transposed last two dims: strides (c*h*w, h*w, 1, h)
//   - Fully reversed: strides (1, n, n*c, n*c*h)
//
// For a 2D tensor of shape (m, n), a transpose would have strides (1, m)
// instead of the contiguous strides (n, 1).
//
// Essentially, strides enable arbitrary transpositions and permutations of
// tensor dimensions while preserving the same logical layout.
//
// PyTorch enforces tensors to always conform to the logical channels-first
// (NCHW) layout, and uses strides as a way to specify the exact physical
// (in-memory) layout - contiguous (NCHW) or channels-last (NHWC). Not all
// operators support the non-contiguous channels-last format though as this
// is specific to the backend that implements it.
//
// On the other hand, there is no notion of strides in MLIR / IREE compiler.
// It expects the tensors to always be contiguous (row-major) as specified by
// its shape. It therefore assumes physical layout is always the same as the
// logical layout.
//
// Fusilli sits between the high-level ML frameworks (PyTorch) and the low-level
// MLIR compilers (IREE), so it needs to conform to the constraints in tensor
// representations between the frontend and the backend while supporting
// various memory formats (for performance reasons). TensorAttr supports any
// valid permutation of dimensions through strides, as long as the stride
// pattern forms a valid physical layout (i.e., each stride is the product of
// dimensions in faster-changing positions). However, individual operations may
// have stricter requirements. For example, convolution operations typically
// only support contiguous (NCHW) and channels-last (NHWC) layouts, while
// pointwise operations support arbitrary transpositions through their stride
// patterns.
//
// Invalid stride configurations (e.g., strides that don't correspond to any
// valid permutation) or strides unsupported by a specific operation will result
// in an error during validation.
//
// In the case of convolution operations, Fusilli emits MLIR assembly for the
// `torch` dialect which is tied to PyTorch semantics where conv ops always
// operate on channels-first (NCHW if 4D) tensors. This works fine when inputs
// to a Fusilli node are channels-first, but we need special handling for when
// they are in channels-last format. We introduce the applicable transposes to
// convert tensors from channels-last (NHWC if 4D) to channels-first (NCHW if
// 4D) at the inputs and vice versa at the outputs of the Fusilli node. This
// ensures that the MLIR assembly generated by Fusilli for that node is
// compatible with the `torch` dialect semantics at the op boundaries. The
// transposes are expected to eventually be propagated and optimized out by the
// IREE compiler. In addition to this, we also transpose the shapes for
// channels-last tensors in the generated MLIR assembly to satisfy the
// contiguity requirement (physical layout == logical layout) in MLIR. This
// allows us to drop the strides in the MLIR assembly.
//
// For example, here's what the initial and subsequent IRs for a convolution on
// a channels-last tensor (NHWC) would look like as emitted by Fusilli and after
// IREE's transpose propagation and elimination passes:
//
//  User input (Fusilli graph):
//    x [NCHW + stride] -> conv [NHWC] -> y [NCHW + stride]
//
//  Generated ASM (Torch dialect):
//    x [NHWC] -> T [NCHW] -> conv [NCHW] -> T' [NHWC] -> y [NHWC]
//
//  Compiler passes (Linalg dialect):
//  Transpose propagation
//    x [NHWC] -> conv [NHWC] -> T [NCHW] -> T' [NHWC] -> y [NHWC]
//
//  Transpose elimination
//    x [NHWC] -> conv [NHWC] -> y [NHWC]
//
//===----------------------------------------------------------------------===//

#ifndef FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H
#define FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H

#include "fusilli/attributes/types.h"
#include "fusilli/graph/context.h"
#include "fusilli/support/logging.h"

#include <algorithm>
#include <cassert>
#include <cstddef>
#include <cstdint>
#include <memory>
#include <numeric>
#include <optional>
#include <ranges>
#include <string>
#include <type_traits>
#include <utility>
#include <variant>
#include <vector>

namespace fusilli {

// Concept that will accept any type that models a range (something with
// .begin(), and .end()) with value type of int64_t.
template <typename R>
concept Int64Range =
    std::ranges::forward_range<R> &&
    std::is_same_v<std::ranges::range_value_t<R>, int64_t>; // C++ 20

// Generates stride order for a contiguous tensor. For a 4D tensor, this would
// return {N: 3, C: 2, H: 1, W: 0} to represent an NCHW in-memory layout.
// Here N is the slowest changing and W is the fastest changing dimension.
inline std::vector<size_t> getContiguousStrideOrder(size_t numDims) {
  assert(numDims >= 1 && "Contiguous layout requires at least 1 dimension");

  std::vector<size_t> strideOrder(numDims);
  size_t order = 0;
  // Caution: Reverse iteration with size_t (unsigned) can lead to underflow
  // when `i == 0` is decremented due to wrap-around to SIZE_MAX (which is
  // positive and doesn't prevent control from entering the loop as intended).
  // This can cause heap corruption when accessing strideOrder[SIZE_MAX].
  // So we instead gate loop entry with `i > 0`.
  for (size_t i = numDims; i > 0; --i)
    strideOrder[i - 1] = order++;
  return strideOrder;
}

// Generates stride order for a channels-last tensor. For a 4D tensor, this
// would return {N: 3, C: 0, H: 2, W: 1} to represent an NHWC in-memory layout.
// Here N is the slowest changing and C is the fastest changing dimension.
inline std::vector<size_t> getChannelsLastStrideOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<size_t> strideOrder(numDims);
  size_t order = 0;
  strideOrder[1] = order++;
  for (size_t i = numDims - 1; i > 1; --i)
    strideOrder[i] = order++;
  strideOrder[0] = order;
  return strideOrder;
}

// Generates a stride order preserving the format of `inputStride`. When the
// desired format has a larger size, the result is padded to be of size
// `outputDimSize`.
//
// For example: an input of {10, 30, 20} would return a stride order of
// {0, 2, 1}.
inline std::vector<size_t>
generateStrideOrderPreservingFormat(const std::vector<int64_t> inputStride,
                                    size_t outputDimSize) {
  std::vector<size_t> indices(inputStride.size());
  std::iota(indices.begin(), indices.end(), 0);

  // Sort indices based on stride values in descending order
  std::sort(indices.begin(), indices.end(), [&inputStride](size_t i, size_t j) {
    return inputStride[i] < inputStride[j];
  });

  // Create the stride order
  std::vector<size_t> strideOrder(inputStride.size());
  for (size_t i = 0; i < indices.size(); ++i) {
    strideOrder[indices[i]] = i;
  }

  // If output_dim_size is larger, pad with remaining dimensions
  if (outputDimSize > inputStride.size()) {
    size_t start = strideOrder.size();
    strideOrder.resize(outputDimSize);
    std::iota(strideOrder.begin() + static_cast<int64_t>(start),
              strideOrder.end(), start);
  }
  return strideOrder;
}

inline std::vector<int64_t>
generateStrideFromDim(const std::vector<int64_t> &dim,
                      const std::vector<size_t> &strideOrder) {
  size_t numDims = dim.size();
  std::vector<int64_t> stride(numDims);
  std::vector<std::pair<size_t, int64_t>> idxToDimInStrideOrder(numDims);

  // Example dim = (10, 3, 12, 12) and strideOrder = {3, 0, 2, 1}
  // idxToDimInStrideOrder will be:
  // { (1, 3), (3, 12), (2, 12), (0, 10) }
  // ordered from fastest changing dim to slowest changing dim
  for (size_t i = 0; i < numDims; ++i)
    idxToDimInStrideOrder[strideOrder[i]] = std::make_pair(i, dim[i]);

  // Stride is cumulative product of dimensions in the order specified by
  // strideOrder. For the above example, this will yield:
  // stride = (432, 1, 36, 3)
  int64_t cumulativeProduct = 1;
  for (size_t i = 0; i < numDims; ++i) {
    stride[idxToDimInStrideOrder[i].first] = cumulativeProduct;
    cumulativeProduct *= idxToDimInStrideOrder[i].second;
  }

  return stride;
}

// Generates permute order to preserve the layout of a contiguous tensor.
// For a 4D tensor, this would return {0, 1, 2, 3} to preserve the NCHW
// layout. This is effectively used for a no-op permute.
inline std::vector<int64_t> getPreserveContiguousPermuteOrder(size_t numDims) {
  assert(numDims >= 1 && "Contiguous layout requires at least 1 dimension");

  std::vector<int64_t> permuteOrder(numDims);
  std::iota(permuteOrder.begin(), permuteOrder.end(), 0);
  return permuteOrder;
}

// Generates permute order to convert the layout of a channels-last tensor to
// a contiguous tensor. For a 4D tensor, this would return {0, 3, 1, 2} to go
// from NHWC to NCHW layout.
inline std::vector<int64_t>
getChannelsLastToContiguousPermuteOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<int64_t> permuteOrder(numDims);
  int64_t order = 0;
  permuteOrder[0] = order++;
  for (size_t i = 2; i < numDims; ++i)
    permuteOrder[i] = order++;
  permuteOrder[1] = order;
  return permuteOrder;
}

// Generates permute order to convert the layout of a contiguous tensor to a
// channels-last tensor. For a 4D tensor, this would return {0, 2, 3, 1} to go
// from NCHW to NHWC layout.
inline std::vector<int64_t>
getContiguousToChannelsLastPermuteOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<int64_t> permuteOrder(numDims);
  int64_t order = 0;
  permuteOrder[0] = order++;
  permuteOrder[numDims - 1] = order++;
  for (size_t i = 1; i < numDims - 1; ++i)
    permuteOrder[i] = order++;
  return permuteOrder;
}

// Given a permutation vector, returns its inverse.
inline std::vector<int64_t>
inversePermutation(const std::vector<int64_t> &permutation) {
  std::vector<int64_t> inverse(permutation.size());
  for (size_t i = 0; i < permutation.size(); ++i) {
    inverse[permutation[i]] = static_cast<int64_t>(i);
  }
  return inverse;
}

// Takes a set of input shapes and computes a common shape that all inputs
// shapes can be broadcast to. This implements Pytorch style broadcasting where
// shapes are right-aligned. For example:
//
// Input shapes:
//   {64, 16,  1, 1}
//       { 1, 32, 1}
//
// Result:
//   {64, 16, 32, 1}
inline ErrorOr<std::vector<int64_t>>
computeBroadcastShape(const std::vector<std::vector<int64_t>> &shapes) {
  // Remove empty shapes.
  auto filteredShapes =
      shapes | std::views::filter([](const std::vector<int64_t> &shape) {
        return !shape.empty();
      });
  FUSILLI_RETURN_ERROR_IF(filteredShapes.empty(), ErrorCode::InvalidAttribute,
                          "All input shapes are empty");

  // Find the maximum rank in `shapes`.
  size_t maxSize =
      std::max_element(
          filteredShapes.begin(), filteredShapes.end(),
          [](const std::vector<int64_t> &lhs, const std::vector<int64_t> &rhs) {
            return lhs.size() < rhs.size();
          })
          ->size();

  std::vector<int64_t> commonShape(maxSize, 1);
  for (const std::vector<int64_t> &shape : filteredShapes) {
    // When broadcasting shapes of differing ranks, the dimensions are
    // right-aligned. Process from rightmost dimension to leftmost.
    for (size_t offset = 0; offset < shape.size(); ++offset) {
      size_t commonIdx = commonShape.size() - 1 - offset;
      size_t shapeIdx = shape.size() - 1 - offset;

      if (commonShape[commonIdx] == 1) {
        commonShape[commonIdx] = shape[shapeIdx];
      }

      FUSILLI_RETURN_ERROR_IF((shape[shapeIdx] != 1) &&
                                  (commonShape[commonIdx] != shape[shapeIdx]),
                              ErrorCode::InvalidAttribute,
                              "Cannot broadcast two non unit dimensions");
    }
  }
  return ok(std::move(commonShape));
}

class TensorAttr {
public:
  using scalar_t = std::variant<int64_t, int32_t, float, double>;

  ErrorObject validate() const {
    FUSILLI_LOG_LABEL_ENDL("INFO: Validating tensor '" << name_ << "'");

    FUSILLI_RETURN_ERROR_IF(dim_.empty(), ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' dims not set");

    FUSILLI_RETURN_ERROR_IF(stride_.empty(), ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' strides not set");

    FUSILLI_RETURN_ERROR_IF(
        dim_.size() != stride_.size(), ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' uses dim and stride of different dimensionality");
    FUSILLI_RETURN_ERROR_IF(
        !hasValidPhysicalRepresentation(), ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' has invalid physical representation. It must be a valid "
            "permutation of the logical dimensions");

    FUSILLI_RETURN_ERROR_IF(dataType_ == DataType::NotSet,
                            ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' data type not set");

    FUSILLI_RETURN_ERROR_IF(
        isVirtual_ && isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' cannot be both virtual (intermediate) and a scalar constant");

    FUSILLI_RETURN_ERROR_IF(
        scalarValue_.has_value() && !isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' has a scalar value set but is not marked as a scalar");

    FUSILLI_RETURN_ERROR_IF(
        !scalarValue_.has_value() && isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' is marked as a scalar but does not have a scalar value set");

    return ok();
  }

  TensorAttr() = default;

  // Constructors for scalar values:
  explicit TensorAttr(float value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Float;
  }

  explicit TensorAttr(double value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Double;
  }

  explicit TensorAttr(int32_t value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Int32;
  }

  explicit TensorAttr(int64_t value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Int64;
  }

  // Fill datatypes from overall context when not set.
  TensorAttr &fillFromContext(const Context &context) {
    if (getDataType() == DataType::NotSet) {
      if (isVirtual())
        setDataType(context.getIntermediateDataType());
      else
        setDataType(context.getIODataType());
    }
    return *this;
  }

  // MLIR assembly emitter helper methods:
  std::string getTensorTypeAsm(bool isValueTensor = true,
                               bool useLogicalDims = false) const;
  std::string getValueNameAsm(bool isOutputAliased = false) const;

  // Setters:
  TensorAttr &setName(const std::string &name) {
    name_ = name;
    return *this;
  }

  TensorAttr &setDataType(DataType dataType) {
    dataType_ = dataType;
    return *this;
  }

  TensorAttr &setDim(const std::vector<int64_t> &dim) {
    dim_ = dim;
    return *this;
  }
  template <Int64Range R> TensorAttr &setDim(R &&dim) {
    dim_.assign(dim.begin(), dim.end());
    return *this;
  }

  TensorAttr &setStride(const std::vector<int64_t> &stride) {
    stride_ = stride;
    return *this;
  }
  template <Int64Range R> TensorAttr &setStride(R &&stride) {
    stride_.assign(stride.begin(), stride.end());
    return *this;
  }

  TensorAttr &setIsVirtual(bool isVirtual) {
    isVirtual_ = isVirtual;
    return *this;
  }

  TensorAttr &setOutput(bool isOutput) { return setIsVirtual(!isOutput); }

  TensorAttr &setIsScalar(bool isScalar) {
    isScalar_ = isScalar;
    return *this;
  }

  // Getters:
  const std::string &getName() const { return name_; }

  DataType getDataType() const { return dataType_; }

  const std::vector<int64_t> &getDim() const { return dim_; }

  const std::vector<int64_t> &getStride() const { return stride_; }

  int64_t getVolume() const {
    int64_t volume = 1;
    for (const auto &d : dim_)
      volume *= d;
    return volume;
  }

  bool isVirtual() const { return isVirtual_; }

  bool isScalar() const { return isScalar_; }

  bool isContiguous() const {
    std::vector<int64_t> expectedStride =
        generateStrideFromDim(dim_, getContiguousStrideOrder(dim_.size()));
    return expectedStride == stride_;
  }

  bool isChannelsLast() const {
    std::vector<int64_t> expectedStride =
        generateStrideFromDim(dim_, getChannelsLastStrideOrder(dim_.size()));
    return expectedStride == stride_;
  }

  // Check if the stride pattern is valid and can represent a physical tensor.
  // A valid stride pattern must satisfy:
  // 1. Each stride is a product of dimensions in faster-changing positions
  // 2. The strides are consistent with some permutation of dimensions
  bool hasValidPhysicalRepresentation() const {
    size_t numDims = dim_.size();
    if (numDims != stride_.size())
      return false;
    if (numDims == 0)
      return true;

    // Create pairs of (stride, dimSize) and sort by stride
    std::vector<std::pair<int64_t, int64_t>> strideAndDim;
    for (size_t i = 0; i < numDims; ++i) {
      if (stride_[i] < 0)
        return false; // Negative strides not supported
      strideAndDim.push_back({stride_[i], dim_[i]});
    }
    std::stable_sort(
        strideAndDim.begin(), strideAndDim.end(),
        [](const auto &a, const auto &b) { return a.first < b.first; });

    // The smallest stride should divide evenly into all larger strides
    // and the expected product of dimensions should match
    int64_t expectedStride = 1;
    for (size_t i = 0; i < numDims; ++i) {
      int64_t actualStride = strideAndDim[i].first;
      int64_t dimSize = strideAndDim[i].second;

      // For unit length dimensions, any stride value is technically valid
      if (dimSize == 1)
        continue;

      // Check if actual stride matches expected stride
      if (actualStride != expectedStride)
        return false;

      expectedStride = actualStride * dimSize;
    }

    return true;
  }

  // Convert logical dims + stride into physical dims. The stride
  // represents how dimensions are accessed in physical memory.
  //
  // Dimensions of size 1 don't meaningfully contribute to the physical layout
  // and are kept in their relative positions to preserve information.
  //
  // Examples:
  //   1. Contiguous NCHW layout: dim={2, 3, 4}, stride={12, 4, 1}
  //      Returns: {2, 3, 4} (no reordering needed)
  //
  //   2. Channels-last NHWC: dim={2, 3, 4}, stride={12, 1, 3}
  //      Returns: {2, 4, 3} (last two dims swapped in physical memory)
  //
  //   3. Fully reversed layout: dim={2, 3, 4}, stride={1, 2, 6}
  //      Returns: {4, 3, 2} (completely reversed in physical memory)
  //
  //   4. With unit dimensions: dim={1, 64, 1, 128}, stride={999, 128, 999, 1}
  //      Returns: {1, 64, 1, 128} (unit dims stay; non-unit dims maintain
  //      order)
  //
  //   5. Complex 4D permutation: dim={2, 3, 4, 5}, stride={60, 1, 15, 3}
  //      Returns: {2, 4, 5, 3} (NCHW logical -> NHWC physical)
  std::vector<int64_t> getPhysicalDim() const {
    assert(hasValidPhysicalRepresentation() &&
           "Tensor has invalid physical representation");
    auto permuteOrder = getLogicalToPhysicalPermuteOrder();
    const size_t numDims = dim_.size();
    std::vector<int64_t> physicalDims(numDims);
    for (size_t i = 0; i < numDims; ++i)
      physicalDims[i] = dim_[permuteOrder[i]];
    return physicalDims;
  }

  // This computes the permutation needed to go from logical dims to physical
  // dims based on the dims and stride. Unit length dimensions are kept in their
  // original positions as they don't meaningfully contribute to the layout.
  //
  // Note: given a permutation vector `perm`, `perm[i]` means that the i-th
  // dimension in the physical layout is mapped to the `perm[i]`-th dimension in
  // the logical layout.
  //
  // Examples:
  //   1. Contiguous NCHW layout: dim={2, 3, 4}, stride={12, 4, 1}
  //      Returns: {0, 1, 2} (identity - already in physical order)
  //
  //   2. Channels-last NHWC: dim={2, 3, 4}, stride={12, 1, 3}
  //      Returns: {0, 2, 1} (swap the last two dimensions)
  //
  //   3. Fully reversed layout: dim={2, 3, 4}, stride={1, 2, 6}
  //      Returns: {2, 1, 0} (complete reversal)
  //
  //   4. With unit dimensions: dim={1, 64, 1, 128}, stride={999, 128, 999, 1}
  //      Returns: {0, 1, 2, 3} (unit dims at 0,2 stay put; non-unit dims keep
  //      order)
  //
  //   5. Complex 4D permutation: dim={2, 3, 4, 5}, stride={60, 1, 15, 3}
  //      Returns: {0, 2, 3, 1} (channels-last style for 4D)
  std::vector<int64_t> getLogicalToPhysicalPermuteOrder() const {
    size_t numDims = dim_.size();
    assert(numDims >= 1 && "Dims must have at least 1 dimension");
    assert(numDims == stride_.size() && "Dims and stride must have same size");

    // Start with identity permutation
    std::vector<int64_t> permuteOrder(numDims);
    std::iota(permuteOrder.begin(), permuteOrder.end(), 0);

    // Collect only non-unit dimensions for reordering
    std::vector<std::pair<int64_t, size_t>> strideWithIndex;
    std::vector<size_t> nonUnitDimIndices;
    for (size_t i = 0; i < numDims; ++i) {
      if (dim_[i] != 1) {
        strideWithIndex.push_back({stride_[i], i});
        nonUnitDimIndices.push_back(i);
      }
    }

    // If all dimensions are 1, return identity permutation
    if (strideWithIndex.empty())
      return permuteOrder;

    // Sort non-unit dimensions by stride value (descending for contiguous
    // order) Larger stride = slower changing = leftmost in contiguous layout
    std::stable_sort(
        strideWithIndex.begin(), strideWithIndex.end(),
        [](const auto &a, const auto &b) { return a.first > b.first; });

    // Build permutation for non-unit dimensions only
    // The sorted order gives us the target positions for contiguous layout
    for (size_t i = 0; i < nonUnitDimIndices.size(); ++i) {
      size_t logicalIdx = nonUnitDimIndices[i];
      size_t targetIdx = strideWithIndex[i].second;
      permuteOrder[logicalIdx] = static_cast<int64_t>(targetIdx);
    }

    return permuteOrder;
  }

  // This computes the permutation needed to go from physical dims to logical
  // dims. This is the inverse of getLogicalToPhysicalPermuteOrder().
  std::vector<int64_t> getPhysicalToLogicalPermuteOrder() const {
    return inversePermutation(getLogicalToPhysicalPermuteOrder());
  }

  std::optional<scalar_t> getScalarValue() const { return scalarValue_; }

private:
  std::string name_;
  DataType dataType_ = DataType::NotSet;
  std::vector<int64_t> dim_ = {};
  std::vector<int64_t> stride_ = {};

  // Intermediate tensors that are not inputs/outputs are virtual
  // and not stored/read as they appear internal to the kernel.
  // They also don't need their shapes and sizes specified.
  bool isVirtual_ = false;

  // To represent scalar constants either obtained through
  // constant folding, or passed in as scalars during execution.
  bool isScalar_ = false;
  std::optional<scalar_t> scalarValue_ = std::nullopt;
};

// Sorting function for deterministic lookups on TensorAttr containers
// (`std::set`) ensuring iteration orders are deterministic. It sorts
// by name.
struct TensorAttrSortByName {
  bool operator()(const std::shared_ptr<TensorAttr> &a,
                  const std::shared_ptr<TensorAttr> &b) const {
    return a->getName() < b->getName();
  }
};

} // namespace fusilli

#endif // FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H
